# RapidSummarize ğŸ“„âš¡

**RapidSummarize** is a smart **Retrieval-Augmented Generation (RAG)** application that lets you upload PDFs and ask questions about them.  
It combines a **React** frontend with a **FastAPI** backend, uses **Ollama** (Llama 3.2 + embeddings) for AI, and **ChromaDB** for vector storage.

Everything runs locally in **Docker** â€” just spin it up and start chatting with your documents.

---

## âœ¨ Features

- ğŸ“ Upload multiple PDFs at once  
- ğŸ” Intelligent text extraction (OCR for scanned pages)  
- ğŸ§  Context-aware conversations (remembers previous messages)  
- âš¡ Real-time streaming responses  
- ğŸ—‚ï¸ File management: select, delete, clear context  
- ğŸ³ Fully containerized with Docker Compose  
- ğŸ” Persistent storage for uploads, vector DB, and AI models  

---

## ğŸ› ï¸ Built With

### Frontend
- React + Vite  
- Served via Nginx  

### Backend
- FastAPI  
- PyMuPDF  
- pytesseract  
- ChromaDB  
- Ollama Python client  

### AI & Storage
- **LLM:** Llama 3.2 (via Ollama)  
- **Embeddings:** nomic-embed-text  
- **Vector DB:** ChromaDB (persistent)  

### Containerization
- Docker  
- Docker Compose  

---

## ğŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose installed  
- **8+ GB RAM recommended**

---

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/VeeSeven/RapidSummarize.git
cd rapidsummarize
````

---

### 2ï¸âƒ£ Start the application

```bash
docker-compose up -d
```

This will:

* Build all images
* Pull and start Ollama
* Launch frontend & backend containers

âš ï¸ **First run note:**
Required models are downloaded automatically and may take **10â€“30 minutes** depending on your internet speed.

---

### 3ï¸âƒ£ Open the app

```
http://localhost
```

---

### 4ï¸âƒ£ Stop the app

```bash
docker-compose down
```

---

## ğŸ”§ Manual Model Pull (Optional)

If models fail to download automatically:

```bash
docker exec -it ollama ollama pull llama3.2
docker exec -it ollama ollama pull nomic-embed-text
```

---

## ğŸ“– How to Use

1. **Upload PDFs**

   * Select one or more PDFs on the home page
   * Optionally enter an initial question

2. **Chat with your documents**

   * Youâ€™ll be redirected to the chat page
   * Choose which PDFs to query via the sidebar

3. **Ask questions**

   * Type a question and press **Enter**
   * Answers are streamed in real time
   * Responses are based **only** on selected PDFs

4. **Manage files & context**

   * Delete PDFs
   * Clear chat history
   * Switch document context anytime

---

## ğŸ”Œ API Overview

| Method | Endpoint             | Description                       |
| ------ | -------------------- | --------------------------------- |
| POST   | `/upload`            | Upload PDFs (multipart/form-data) |
| GET    | `/files`             | List all uploaded PDFs            |
| DELETE | `/files/{filename}`  | Delete a PDF and its vectors      |
| POST   | `/chat`              | Query selected PDFs               |
| POST   | `/chat-with-context` | Query with previous chat context  |

All responses are returned as **JSON** or **streaming text**.

---

## âš™ï¸ Configuration

Set the following in `docker-compose.yml`:

| Variable          | Default               | Description                    |
| ----------------- | --------------------- | ------------------------------ |
| `ALLOWED_ORIGINS` | `http://localhost`    | CORS origins (comma-separated) |
| `OLLAMA_HOST`     | `http://ollama:11434` | Internal Ollama service URL    |

### Frontend API

* Set at build time using `VITE_API_URL`
* Default: `http://localhost:8000`

---

## ğŸ’¾ Persistent Volumes

| Volume         | Purpose               |
| -------------- | --------------------- |
| `ollama_data`  | AI models             |
| `uploads_data` | Uploaded PDFs         |
| `chroma_data`  | ChromaDB vector index |

---

## ğŸ§ª Development (Without Docker)

### Backend

```bash
cd backend
pip install -r requirements.txt
uvicorn main:app --reload
```

### Frontend

```bash
cd frontend
npm install
npm run dev
```

âš ï¸ Make sure **Ollama** is installed locally and running with the required models.

---

## ğŸ™ Thanks

* Ollama â€” for making local LLMs easy
* ChromaDB â€” for the vector database
* FastAPI â€” for the backend framework
* React & Vite â€” for frontend tooling

---

**Happy Summarizing!**
